#!/usr/bin/env python3
"""
Phase 1: W8A8 Uniform Quantization for Nemotron Hybrid Models

Conservative quantization approach targeting 50% memory reduction while preserving accuracy.
Integrates MambaQuant for SSM layers and AutoGPTQ for MLP/Attention layers.

Author: Yash Semlani (yashvsemlani@gmail.com)
Date: 2024-12-19
"""

import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModelForCausalLM
from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig
import json
import time
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import sys
import os

# Add MambaQuant to path
sys.path.append('./MambaQuant')

from analyze_nemotron_model import NemotronModelAnalyzer


class Phase1Quantizer:
    """
    Phase 1: Conservative W8A8 uniform quantization implementation
    
    Strategy:
    - SSM layers: MambaQuant W8A8 (specialized for state space models)
    - MLP/Attention layers: AutoGPTQ W8A8 (standard transformer quantization)
    - Target: 50% memory reduction with minimal accuracy loss
    """
    
    def __init__(self, 
                 model_name: str = "nvidia/Nemotron-H-8B-Base-8K",
                 output_dir: str = "./quantized_models/phase1_w8a8"):
        self.model_name = model_name
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Model components
        self.original_model = None
        self.quantized_model = None
        self.tokenizer = None
        
        # Analysis components
        self.analyzer = None
        self.quantization_plan = None
        
        # Results tracking
        self.results = {
            'memory_original_mb': 0,
            'memory_quantized_mb': 0,
            'memory_reduction_percent': 0,
            'quantization_time_seconds': 0,
            'layer_statistics': {},
            'errors': []
        }
    
    def load_original_model(self) -> None:
        """Load the original FP16 model and analyze its structure"""
        print("üîç Loading original Nemotron model for analysis...")
        
        try:
            # Load tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name, 
                trust_remote_code=True
            )
            
            # Load model in FP16
            self.original_model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True
            )
            
            print(f"‚úÖ Original model loaded successfully")
            print(f"   Model class: {self.original_model.__class__.__name__}")
            print(f"   Device map: {getattr(self.original_model, 'hf_device_map', 'Not available')}")
            
            # Estimate original memory usage
            self.results['memory_original_mb'] = self._estimate_model_memory(self.original_model)
            print(f"   Estimated memory: {self.results['memory_original_mb']:.2f} MB")
            
        except Exception as e:
            error_msg = f"Failed to load original model: {e}"
            self.results['errors'].append(error_msg)
            print(f"‚ùå {error_msg}")
            raise
    
    def analyze_model_structure(self) -> Dict[str, Any]:
        """Analyze model structure and create quantization plan"""
        print("üîç Analyzing model structure for quantization planning...")
        
        try:
            # Initialize analyzer
            self.analyzer = NemotronModelAnalyzer(self.model_name)
            self.analyzer.model = self.original_model  # Use already loaded model
            self.analyzer.tokenizer = self.tokenizer
            
            # Analyze all layers
            self.analyzer.analyze_all_layers()
            
            # Generate quantization plan
            self.quantization_plan = self.analyzer.get_quantization_plan()
            
            # Store layer statistics
            self.results['layer_statistics'] = self.quantization_plan['layer_statistics']
            
            print("‚úÖ Model analysis completed")
            print(f"   Total parameters: {self.quantization_plan['layer_statistics']['total_params']:,}")
            print(f"   SSM layers: {self.quantization_plan['layer_statistics']['ssm_layers']['count']}")
            print(f"   MLP layers: {self.quantization_plan['layer_statistics']['mlp_layers']['count']}")
            print(f"   Attention layers: {self.quantization_plan['layer_statistics']['attention_layers']['count']}")
            
            return self.quantization_plan
            
        except Exception as e:
            error_msg = f"Failed to analyze model structure: {e}"
            self.results['errors'].append(error_msg)
            print(f"‚ùå {error_msg}")
            raise
    
    def configure_w8a8_quantization(self) -> BaseQuantizeConfig:
        """Configure W8A8 quantization parameters"""
        print("‚öôÔ∏è Configuring W8A8 quantization parameters...")
        
        # AutoGPTQ configuration for W8A8
        quantize_config = BaseQuantizeConfig(
            bits=8,                    # 8-bit weights
            group_size=128,           # Group size for quantization
            damp_percent=0.1,         # Damping factor
            desc_act=False,           # Disable activation description (use 8-bit activations)
            static_groups=False,      # Dynamic grouping
            sym=True,                 # Symmetric quantization
            true_sequential=True,     # Sequential quantization
            model_name_or_path=None,
            model_file_base_name="model"
        )
        
        print("‚úÖ W8A8 quantization config created:")
        print(f"   Bits: {quantize_config.bits}")
        print(f"   Group size: {quantize_config.group_size}")
        print(f"   Symmetric: {quantize_config.sym}")
        
        return quantize_config
    
    def apply_phase1_quantization(self) -> None:
        """Apply Phase 1 W8A8 quantization to the model"""
        print("\nüéØ Starting Phase 1 W8A8 Quantization...")
        start_time = time.time()
        
        try:
            # Configure quantization
            quantize_config = self.configure_w8a8_quantization()
            
            # TODO: Implement hybrid quantization logic
            # For now, use standard AutoGPTQ as baseline
            print("üì¶ Applying AutoGPTQ W8A8 quantization (baseline implementation)...")
            
            # Note: This is a simplified implementation
            # Full implementation would separate SSM vs MLP/Attention layers
            self.quantized_model = AutoGPTQForCausalLM.from_pretrained(
                self.model_name,
                quantize_config=quantize_config,
                trust_remote_code=True,
                device_map="auto"
            )
            
            # Estimate quantized memory usage
            self.results['memory_quantized_mb'] = self._estimate_model_memory(self.quantized_model)
            
            # Calculate memory reduction
            original_memory = self.results['memory_original_mb']
            quantized_memory = self.results['memory_quantized_mb']
            self.results['memory_reduction_percent'] = (
                (original_memory - quantized_memory) / original_memory * 100
            )
            
            self.results['quantization_time_seconds'] = time.time() - start_time
            
            print(f"‚úÖ Phase 1 quantization completed!")
            print(f"   Original memory: {original_memory:.2f} MB")
            print(f"   Quantized memory: {quantized_memory:.2f} MB")
            print(f"   Memory reduction: {self.results['memory_reduction_percent']:.1f}%")
            print(f"   Quantization time: {self.results['quantization_time_seconds']:.1f} seconds")
            
        except Exception as e:
            error_msg = f"Phase 1 quantization failed: {e}"
            self.results['errors'].append(error_msg)
            print(f"‚ùå {error_msg}")
            raise
    
    def save_quantized_model(self) -> None:
        """Save the quantized model to disk"""
        print(f"üíæ Saving quantized model to {self.output_dir}...")
        
        try:
            # Save quantized model
            self.quantized_model.save_quantized(str(self.output_dir))
            
            # Save tokenizer
            self.tokenizer.save_pretrained(str(self.output_dir))
            
            # Save quantization results
            results_file = self.output_dir / "quantization_results.json"
            with open(results_file, 'w') as f:
                json.dump(self.results, f, indent=2)
            
            print(f"‚úÖ Model saved successfully to {self.output_dir}")
            
        except Exception as e:
            error_msg = f"Failed to save quantized model: {e}"
            self.results['errors'].append(error_msg)
            print(f"‚ùå {error_msg}")
            raise
    
    def _estimate_model_memory(self, model: nn.Module) -> float:
        """Estimate model memory usage in MB"""
        total_params = sum(p.numel() for p in model.parameters())
        # Assume 2 bytes per parameter for FP16/INT8
        bytes_per_param = 2
        total_bytes = total_params * bytes_per_param
        return total_bytes / (1024 * 1024)  # Convert to MB
    
    def validate_quantized_model(self) -> bool:
        """Basic validation of quantized model functionality"""
        print("üîç Validating quantized model functionality...")
        
        try:
            # Simple inference test
            test_prompt = "The capital of France is"
            inputs = self.tokenizer(test_prompt, return_tensors="pt")
            
            # Move inputs to model device
            device = next(self.quantized_model.parameters()).device
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # Generate response
            with torch.no_grad():
                outputs = self.quantized_model.generate(
                    **inputs,
                    max_new_tokens=10,
                    do_sample=False,
                    temperature=1.0
                )
            
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            print(f"‚úÖ Validation successful. Test response: '{response}'")
            return True
            
        except Exception as e:
            error_msg = f"Model validation failed: {e}"
            self.results['errors'].append(error_msg)
            print(f"‚ùå {error_msg}")
            return False
    
    def run_complete_phase1_pipeline(self) -> Dict[str, Any]:
        """Run the complete Phase 1 quantization pipeline"""
        print("üöÄ Starting Phase 1 W8A8 Quantization Pipeline")
        print("=" * 60)
        
        pipeline_start = time.time()
        
        try:
            # Step 1: Load original model
            self.load_original_model()
            
            # Step 2: Analyze model structure
            self.analyze_model_structure()
            
            # Step 3: Apply quantization
            self.apply_phase1_quantization()
            
            # Step 4: Validate quantized model
            if not self.validate_quantized_model():
                raise RuntimeError("Quantized model validation failed")
            
            # Step 5: Save results
            self.save_quantized_model()
            
            total_time = time.time() - pipeline_start
            print(f"\nüéâ Phase 1 pipeline completed successfully!")
            print(f"   Total pipeline time: {total_time:.1f} seconds")
            print(f"   Quantized model saved to: {self.output_dir}")
            
            self.results['total_pipeline_time_seconds'] = total_time
            return self.results
            
        except Exception as e:
            error_msg = f"Phase 1 pipeline failed: {e}"
            self.results['errors'].append(error_msg)
            print(f"‚ùå {error_msg}")
            raise


def main():
    """Main execution function for Phase 1 quantization"""
    print("üéØ Phase 1: W8A8 Uniform Quantization")
    print("=" * 50)
    
    # Initialize quantizer
    quantizer = Phase1Quantizer()
    
    try:
        # Run complete pipeline
        results = quantizer.run_complete_phase1_pipeline()
        
        print(f"\nüìä Final Results Summary:")
        print(f"   Memory reduction: {results['memory_reduction_percent']:.1f}%")
        print(f"   Quantization time: {results['quantization_time_seconds']:.1f}s")
        
        if results['errors']:
            print(f"   Errors encountered: {len(results['errors'])}")
            for error in results['errors']:
                print(f"     - {error}")
        
    except Exception as e:
        print(f"‚ùå Phase 1 quantization failed: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main()) 